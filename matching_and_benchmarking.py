import numpy as np
import pandas as pd
from scipy.spatial import cKDTree
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
import sys

# =========================================================
# 1. SETUP & INPUTS
# =========================================================

# Inputs
groundtruth_detections_path = snakemake.input.groundtruth_detections
groundtruth_classes_path    = snakemake.input.groundtruth_classifications
detected_points_path        = snakemake.input.detected_points
classified_points_path      = snakemake.input.classified_points

# Outputs
class_benchmarks_outputpath  = snakemake.output.class_benchmarks
global_benchmarks_outputpath = snakemake.output.global_benchmarks

# Parameters
matching_threshold = snakemake.params.matching_threshold
sim_config         = snakemake.params.sim_settings

# =========================================================
# 2. DYNAMIC CLASS PARSING
# =========================================================

# Map the keys in config.yaml to the fixed IDs generated by data_simulation.py
# (0=Stationary, 1=Aggregate, 2=Moving)
# Could (and maybe should) make this a variable from inside the the config
CONFIG_KEY_TO_ID = {
    "stationary": 0,
    "aggregate":  1,
    "moving":     2
}

classes = []
class_names = {}

# Loop through the known keys to see what is active in the config
for key, class_id in CONFIG_KEY_TO_ID.items():
    # Check if section exists and is activated in the yaml
    if key in sim_config and sim_config[key].get("activate", False):
        classes.append(class_id)
        display_name = sim_config[key].get("display_name", key.title())
        class_names[class_id] = display_name

print(f"Benchmarking against active classes: {class_names}")

# =========================================================
# 3. LOAD DATA
# =========================================================

try:
    groundtruth_detections      = np.load(groundtruth_detections_path)
    groundtruth_classifications = np.load(groundtruth_classes_path)
    detected_points             = np.load(detected_points_path)
    classified_points           = np.load(classified_points_path)
except Exception as e:
    print(f"Error loading numpy arrays: {e}")
    sys.exit(1)

# =========================================================
# 4. MATCHING LOGIC
# =========================================================


def hungarian_matching(points_gt, points_det, max_dist):
    """
    Globally optimal matching using the Hungarian Algorithm (Kuhn-Munkres).
    Matches Ground Truth (A) to Detections (B) by minimizing total Euclidean distance.
    
    Includes a "Cost Penalty" strategy (Jaqaman et al., 2008) to prevents 
    outliers from skewing the optimization.
    
    Returns:
        matched_pairs: List of tuples (idx_gt, idx_det)
        unmatched_gt:  List of indices in points_gt
        unmatched_det: List of indices in points_det
    """
    if len(points_gt) == 0:
        return [], [], list(range(len(points_det)))
    if len(points_det) == 0:
        return [], list(range(len(points_gt))), []

    # 1. Calculate Cost Matrix (Euclidean Distance)
    # Shape: (n_gt, n_det)
    cost_matrix = cdist(points_gt, points_det)

    # 2. Apply "Penalty" for Impossible Matches (Pre-Gating)
    # Set distances > max_dist to a penalty value larger than any possible valid match.
    penalty_value = 1e6 
    cost_matrix[cost_matrix > max_dist] = penalty_value

    # 3. Hungarian Algorithm (Global Optimization)
    # Finds the row_ind and col_ind that minimize the total cost of the matrix.
    row_ind, col_ind = linear_sum_assignment(cost_matrix)

    # 4. Final Validation (Post-Gating)
    # Filter out matches that were assigned the "penalty" cost or exceed the threshold.
    matched_pairs = []
    used_gt = set()
    used_det = set()

    for r, c in zip(row_ind, col_ind):
        # Retrieve the original distance (re-calculating to be safe, or use original matrix)
        dist = np.linalg.norm(points_gt[r] - points_det[c])
        
        if dist <= max_dist:
            matched_pairs.append((r, c))
            used_gt.add(r)
            used_det.add(c)
            
    # 5. Calculate Unmatched lists
    all_gt_indices = set(range(len(points_gt)))
    all_det_indices = set(range(len(points_det)))
    
    unmatched_gt = list(all_gt_indices - used_gt)
    unmatched_det = list(all_det_indices - used_det)
            
    return matched_pairs, unmatched_gt, unmatched_det


# =========================================================
# MATCHING
# =========================================================

#Detections
matches_raw_list, unpaired_gt_raw, unpaired_det_raw = hungarian_matching(
    groundtruth_detections, 
    detected_points, 
    matching_threshold
)
matches_raw = np.array(matches_raw_list)

if matches_raw.size > 0:
    # matches_raw[:, 0] are indices of the Ground Truth array
    matched_gt_indices_raw = matches_raw[:, 0]
    matched_classes_raw = groundtruth_classifications[matched_gt_indices_raw]
else:
    matched_classes_raw = np.array([])


#Selections
matches_filt_list, unpaired_gt_filt, unpaired_det_filt = hungarian_matching(
    groundtruth_detections, 
    classified_points, 
    matching_threshold
)
matches_filt = np.array(matches_filt_list)

if matches_filt.size > 0:
    matched_gt_indices_filt = matches_filt[:, 0]
    matched_classes_filt = groundtruth_classifications[matched_gt_indices_filt]
else:
    matched_classes_filt = np.array([])

# =========================================================
# 6. CALCULATE METRICS
# =========================================================

def safe_div(n, d):
    return n / d if d > 0 else 0.0

performance_data = []

# --- Per-Class Statistics ---
for c_id in classes:
    name = class_names[c_id]
    
    # Total GT: How many of this class actually exist?
    total_gt_count = np.sum(groundtruth_classifications == c_id)
    
    # Matches: How many of this class did we find?
    det_raw_count  = np.sum(matched_classes_raw == c_id)
    det_filt_count = np.sum(matched_classes_filt == c_id)
    
    performance_data.append({
        "Class": name,
        "Raw_Count": det_raw_count,
        "Filtered_Count": det_filt_count,
        "Retention": safe_div(det_filt_count, det_raw_count),
        "Recall_Raw": safe_div(det_raw_count, total_gt_count),
        "Recall_Filtered": safe_div(det_filt_count, total_gt_count)
    })

# --- Noise Statistics ---
# "Pure Noise" are detections that matched NOTHING in the Ground Truth
performance_data.append({
    "Class": "Pure Noise",
    "Raw_Count": len(unpaired_det_raw),
    "Filtered_Count": len(unpaired_det_filt),
    "Retention": safe_div(len(unpaired_det_filt), len(unpaired_det_raw)),
    "Recall_Raw": 0.0,      
    "Recall_Filtered": 0.0 
})

df_classes = pd.DataFrame(performance_data)

# --- Global Statistics ---
# Stationary is the target
# Precision = (Matches of Class 0) / (Total Detection)

total_raw_dets_count = len(detected_points)
total_filt_dets_count = len(classified_points)


stationary_name = class_names.get("Stationary")
stat_row = df_classes.loc[df_classes['Class'] == stationary_name]

if not stat_row.empty:
    tp_raw      = stat_row['Raw_Count'].values[0]
    tp_filt     = stat_row['Filtered_Count'].values[0]
    recall_raw  = stat_row['Recall_Raw'].values[0]
    recall_filt = stat_row['Recall_Filtered'].values[0]
else:
    tp_raw, tp_filt, recall_raw, recall_filt = 0, 0, 0, 0

# Precision
prec_raw  = safe_div(tp_raw, total_raw_dets_count)
prec_filt = safe_div(tp_filt, total_filt_dets_count)

# F1 Score
f1_raw  = safe_div(2 * (prec_raw * recall_raw), (prec_raw + recall_raw))
f1_filt = safe_div(2 * (prec_filt * recall_filt), (prec_filt + recall_filt))

df_global = pd.DataFrame({
    "Raw": [prec_raw, recall_raw, f1_raw],
    "Filtered": [prec_filt, recall_filt, f1_filt]
}, index=["Global_Precision", "Global_Recall", "Global_F1_Score"])

# =========================================================
# 7. SAVE OUTPUTS
# =========================================================

df_classes.to_csv(class_benchmarks_outputpath, index=False) 
df_global.to_csv(global_benchmarks_outputpath)

print(f"Benchmarking complete. Saved to results folder.")
