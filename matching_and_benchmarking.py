# scripts/data_analysis/matching_and_benchmarking.py

import numpy as np
import pandas as pd
from scipy.spatial import cKDTree
import sys

# =========================================================
# 1. SETUP & INPUTS
# =========================================================

# Inputs
groundtruth_detections_path = snakemake.input.groundtruth_detections
groundtruth_classes_path    = snakemake.input.groundtruth_classifications
detected_points_path        = snakemake.input.detected_points
classified_points_path      = snakemake.input.classified_points

# Outputs
class_benchmarks_outputpath  = snakemake.output.class_benchmarks
global_benchmarks_outputpath = snakemake.output.global_benchmarks

# Parameters
matching_threshold = snakemake.params.matching_threshold
sim_config         = snakemake.params.sim_settings

# =========================================================
# 2. DYNAMIC CLASS PARSING
# =========================================================

# Map the keys in config.yaml to the fixed IDs generated by data_simulation.py
# (0=Stationary, 1=Aggregate, 2=Moving)
CONFIG_KEY_TO_ID = {
    "stationary": 0,
    "aggregate":  1,
    "moving":     2
}

classes = []
class_names = {}

# Loop through the known keys to see what is active in the config
for key, class_id in CONFIG_KEY_TO_ID.items():
    # Check if section exists and is activated in the yaml
    if key in sim_config and sim_config[key].get("activate", False):
        classes.append(class_id)
        # Get custom display name from config, default to Title Case (e.g. "Stationary")
        display_name = sim_config[key].get("display_name", key.title())
        class_names[class_id] = display_name

# Fallback: If config parsing fails (e.g. older config version), use defaults
if not classes:
    print("Warning: No active classes found in config parsing. Using default classes.")
    classes = [0, 1, 2]
    class_names = {0: "Stationary", 1: "Aggregate", 2: "Moving"}

print(f"Benchmarking against active classes: {class_names}")

# =========================================================
# 3. LOAD DATA
# =========================================================

try:
    groundtruth_detections      = np.load(groundtruth_detections_path)
    groundtruth_classifications = np.load(groundtruth_classes_path)
    detected_points             = np.load(detected_points_path)
    classified_points           = np.load(classified_points_path)
except Exception as e:
    print(f"Error loading numpy arrays: {e}")
    sys.exit(1)

# =========================================================
# 4. MATCHING LOGIC
# =========================================================

def fast_greedy_matching(points_gt, points_det, max_dist):
    """
    Optimized greedy matching using cKDTree.
    Matches Ground Truth (A) to Detections (B).
    
    Returns:
        matched_pairs: List of tuples (idx_gt, idx_det)
        unmatched_gt:  List of indices in points_gt
        unmatched_det: List of indices in points_det
    """
    # Handle empty inputs gracefully
    if len(points_gt) == 0:
        return [], [], list(range(len(points_det)))
    if len(points_det) == 0:
        return [], list(range(len(points_gt))), []

    # 1. Build a KD-Tree for the Detections (target)
    tree = cKDTree(points_det)
    
    # 2. Query for the nearest neighbor in Detections for every GT point
    # k=1 returns the single closest neighbor
    dists, indices = tree.query(points_gt, k=1, distance_upper_bound=max_dist)
    
    # 3. Filter out points that didn't find a neighbor within max_dist
    # (cKDTree returns infinity for distances > max_dist)
    valid_mask = dists != float('inf')
    
    candidate_gt = np.where(valid_mask)[0]
    candidate_det = indices[valid_mask]
    candidate_dists = dists[valid_mask]
    
    # 4. Sort ONLY the valid candidates by distance (Ascending)
    # This is crucial for greedy matching: closest pairs get priority.
    sorted_indices = np.argsort(candidate_dists)
    
    candidate_gt = candidate_gt[sorted_indices]
    candidate_det = candidate_det[sorted_indices]
    
    # 5. Greedy Assignment
    matched_pairs = []
    used_det = set()      # Track used detection indices
    matched_gt_set = set() # Track used GT indices

    for gt_idx, det_idx in zip(candidate_gt, candidate_det):
        # If this detection point hasn't been claimed by a closer GT point
        if det_idx not in used_det:
            matched_pairs.append((gt_idx, det_idx))
            used_det.add(det_idx)
            matched_gt_set.add(gt_idx)
            
    # 6. Calculate Unmatched lists
    all_gt_indices = set(range(len(points_gt)))
    all_det_indices = set(range(len(points_det)))
    
    unmatched_gt = list(all_gt_indices - matched_gt_set)
    unmatched_det = list(all_det_indices - used_det)
            
    return matched_pairs, unmatched_gt, unmatched_det

# =========================================================
# 5. EXECUTE MATCHING
# =========================================================

# --- Analysis A: RAW Detections ---
matches_raw_list, unpaired_gt_raw, unpaired_det_raw = fast_greedy_matching(
    groundtruth_detections, 
    detected_points, 
    matching_threshold
)
matches_raw = np.array(matches_raw_list)

if matches_raw.size > 0:
    # matches_raw[:, 0] are indices of the Ground Truth array
    matched_gt_indices_raw = matches_raw[:, 0].astype(int)
    matched_classes_raw = groundtruth_classifications[matched_gt_indices_raw]
else:
    matched_classes_raw = np.array([])


# --- Analysis B: FILTERED Detections ---
matches_filt_list, unpaired_gt_filt, unpaired_det_filt = fast_greedy_matching(
    groundtruth_detections, 
    classified_points, 
    matching_threshold
)
matches_filt = np.array(matches_filt_list)

if matches_filt.size > 0:
    matched_gt_indices_filt = matches_filt[:, 0].astype(int)
    matched_classes_filt = groundtruth_classifications[matched_gt_indices_filt]
else:
    matched_classes_filt = np.array([])

# =========================================================
# 6. CALCULATE METRICS
# =========================================================

def safe_div(n, d):
    return n / d if d > 0 else 0.0

performance_data = []

# --- Per-Class Statistics ---
for c_id in classes:
    name = class_names[c_id]
    
    # Total GT: How many of this class actually exist?
    total_gt_count = np.sum(groundtruth_classifications == c_id)
    
    # Matches: How many of this class did we find?
    det_raw_count  = np.sum(matched_classes_raw == c_id)
    det_filt_count = np.sum(matched_classes_filt == c_id)
    
    performance_data.append({
        "Class": name,
        "Raw_Count": det_raw_count,
        "Filtered_Count": det_filt_count,
        "Retention": safe_div(det_filt_count, det_raw_count),
        "Recall_Raw": safe_div(det_raw_count, total_gt_count),
        "Recall_Filtered": safe_div(det_filt_count, total_gt_count)
    })

# --- Noise Statistics ---
# "Pure Noise" are detections that matched NOTHING in the Ground Truth
performance_data.append({
    "Class": "Pure Noise",
    "Raw_Count": len(unpaired_det_raw),
    "Filtered_Count": len(unpaired_det_filt),
    "Retention": safe_div(len(unpaired_det_filt), len(unpaired_det_raw)),
    "Recall_Raw": 0.0,      
    "Recall_Filtered": 0.0 
})

df_classes = pd.DataFrame(performance_data)

# --- Global Statistics ---
# We treat "Stationary" (ID 0) as the True Positive target.
# Precision = (Matches of Class 0) / (Total Detections made by algo)

total_raw_dets_count = len(detected_points)
total_filt_dets_count = len(classified_points)

# Find the stats for the stationary class (ID 0)
# We rely on ID 0 because the user might have renamed "Stationary" to "Fixed" in config
stationary_name = class_names.get(0, "Stationary") # Fallback to string if 0 not active
stat_row = df_classes.loc[df_classes['Class'] == stationary_name]

if not stat_row.empty:
    tp_raw      = stat_row['Raw_Count'].values[0]
    tp_filt     = stat_row['Filtered_Count'].values[0]
    recall_raw  = stat_row['Recall_Raw'].values[0]
    recall_filt = stat_row['Recall_Filtered'].values[0]
else:
    # If stationary simulation was turned off
    tp_raw, tp_filt, recall_raw, recall_filt = 0, 0, 0, 0

# Precision
prec_raw  = safe_div(tp_raw, total_raw_dets_count)
prec_filt = safe_div(tp_filt, total_filt_dets_count)

# F1 Score
f1_raw  = safe_div(2 * (prec_raw * recall_raw), (prec_raw + recall_raw))
f1_filt = safe_div(2 * (prec_filt * recall_filt), (prec_filt + recall_filt))

df_global = pd.DataFrame({
    "Raw": [prec_raw, f1_raw],
    "Filtered": [prec_filt, f1_filt]
}, index=["Global_Precision", "Global_F1_Score"])

# =========================================================
# 7. SAVE OUTPUTS
# =========================================================

df_classes.to_csv(class_benchmarks_outputpath, index=False) 
df_global.to_csv(global_benchmarks_outputpath)

print(f"Benchmarking complete. Saved to results folder.")